Thanks! I’ll look into current (as of July 2025) semantic embedding models that support efficient truncation or follow the Matryoshka Representation Learning principle—even if they don’t name it explicitly.

I’ll cover both English and multilingual models separately, with a focus on models used in search and general-purpose semantic embeddings. I’ll consider open-source as well as commercial solutions, and draw from all types of sources (papers, repos, blog posts, etc.).

I’ll let you know as soon as I have the findings ready.


# Matryoshka Embeddings in Modern Semantic Models (July 2025 Update)

**Matryoshka Representation Learning (MRL)** is a training technique that produces *nested* or *elastic* embeddings which remain useful even after truncating them to fewer dimensions. In a Matryoshka (or “Russian doll”) model, the most important semantic information is **frontloaded into the earlier dimensions** of the vector. This means we can drop the later components of a high-dimensional embedding and still retain enough meaning for downstream tasks. The benefit is that we get **flexible trade-offs** between accuracy and efficiency: smaller embeddings require less storage and yield faster similarity computations, while larger embeddings preserve full accuracy. This concept enables workflows like *shortlist-and-rerank*, where one first uses short embeddings for quick retrieval, then refines results with full-size vectors.

## English Monolingual Models with MRL

* **Sentence Transformers Matryoshka Models:** The Sentence-Transformers library added support for MRL in 2024, making it easy to fine-tune models that yield variable-length embeddings. For example, a Matryoshka version of MPNet was trained on NLI data (AllNLI) using a combined Matryoshka + MultipleNegativesRanking loss. This **English** model (`tomaarsen/mpnet-base-nli-matryoshka`) outputs 768-dimensional embeddings that can be truncated to 512, 256, 128, etc., with minimal performance loss. In fact, at only \~8% of the original size, it retained \~98.4% of its semantic similarity performance (versus \~96.5% for a standard model). Many similar models have since been fine-tuned with MRL for English texts. For instance, a **biomedical** model based on PubMedBERT was published with Matryoshka training, allowing dynamic sizes 64 through 768 for medical text embeddings. Likewise, a **financial-domain** model fine-tuned from BAAI’s BGE is available in a Matryoshka variant (768→512/256 dimensions). These models demonstrate that even general-purpose or domain-specific embeddings (for semantic similarity, clustering, classification, etc.) can be made *resizable* with MRL, without sacrificing accuracy in their niche.

* **Nested Embeddings in Other Languages:** The Matryoshka principle has also been adopted beyond English in single-language settings. For example, researchers in Arabic NLP introduced “nested” Arabic embedding models using MRL, such as the GATE-Arabert model. This Arabic Matryoshka model achieved state-of-the-art semantic textual similarity scores (STS17 benchmark) while enabling smaller embedding sizes. This underscores that MRL’s efficiency gains (smaller vectors for similar performance) are being realized in various languages and domains, not just English.

* **Multi-Modal Matryoshka (CLIP-based):** Interestingly, the Matryoshka idea extends to multimodal embeddings as well. Marqo AI applied MRL to a CLIP-derived model (called **GCL**, which learns joint image-text embeddings). By training CLIP with Matryoshka loss (e.g. using dimension sets like {512, 256, 128, 64}), they showed the truncated image/text vectors maintain high retrieval performance. A GCL model with MRL suffered only marginal drops in normalized DCG when using smaller embedding sizes, unlike the same model without MRL which degraded much more quickly. In other words, even for complex tasks like cross-modal search, one can train *single* models that output embeddings at multiple resolutions. Importantly, Marqo confirmed that the MRL-trained CLIP model performs on par with the original CLIP at full dimension, so one model can flexibly serve either full or reduced embeddings as needed. This showcases MRL as a general principle: from text to images, embedding models can be taught to compress information hierarchically.

**General vs. Search-Focused:** Many of the above English Matryoshka models were trained on general semantic similarity tasks (e.g. NLI, STS) and can be used for clustering, semantic search, paraphrase mining, etc. (they aim for universal sentence representations). On the other hand, some are tuned for **information retrieval**, which often means they were trained on contrastive search datasets. For instance, the pubmedbert-matryoshka model is intended for semantic search in biomedical literature, and the financial BGE-matryoshka model is evaluated on information retrieval metrics (e.g. it includes IR evaluation results at 768 vs 512 dimensions). In practice, Matryoshka training benefits both scenarios: general-purpose embedding models gain flexibility for faster approximate matching, and *search-oriented* models gain a tool to trade accuracy for speed at query time. The typical use-case is to index and query with a smaller vector, then re-rank top hits using the full vector – an approach explicitly recommended in MRL literature and supported by tools like Vespa and Weaviate for OpenAI’s MRL embeddings.

## Multilingual and Commercial Models with MRL

* **OpenAI’s Embedding Models (text-embedding-3):** In January 2024, OpenAI introduced new embedding models that natively support MRL-style shortening. Their `text-embedding-3-large` model outputs 3,072-dim vectors by default, but the API allows requesting a smaller dimensionality – effectively truncating the embedding – with only a slight accuracy hit. OpenAI confirmed this was achieved via Matryoshka Representation Learning (revealed in a footnote). For example, a 256-dim truncation of *3-large* actually **outperforms** the older 1536-dim ada-002 model on the MTEB benchmark. This large model (and a smaller 768-dim *3-small* model) are **multilingual**, excelling in both English and non-English tasks (MIRACL retrieval scores roughly doubled vs. ada-002). OpenAI’s deployment proved the viability of MRL in production: developers can dial down embedding size on the fly (via an API parameter) to save cost or meet index constraints, while retaining the “concept representing properties” of the original vector. These MRL embeddings have been integrated into many vector databases and search platforms (e.g. Vespa, Weaviate) to enable **adaptive retrieval** – performing coarse search with 512 or 1024 dims and then refining with 3072-dims for maximum accuracy.

* **Alibaba’s GTE Family:** Alibaba released an open multilingual model called **GTE-multilingual-base** that explicitly implements the Matryoshka principle. The model card notes it supports *“elastic output dense representations”* to reduce storage and improve efficiency. GTE-multilingual-base (305M params) produces 768-dim embeddings, but any output size from 128 up to 768 can be selected at inference. This flexibility comes from training with MRL on a large multilingual corpus. Notably, GTE achieved state-of-the-art retrieval performance on multilingual benchmarks. Alibaba also provides an English-optimized variant (e.g. gte-large-en) and even larger instruction-tuned models (like gte-Qwen series), though the encoder-only *base* model is highlighted for its **elastic dense embedding** capability. In short, Alibaba’s open GTE models show that MRL is not limited to small research demos – it’s part of production-grade multilingual embedding solutions. (Another example: BAAI’s **BGE-M3** model supports multi-language and multi-granularity, though its “multi-granularity” refers to input length and multi-vector outputs rather than explicit Matryoshka dims – BGE focuses on other efficiency techniques.)

* **Nomic’s Resizable Embeddings:** Nomic AI introduced *open-source* embedding models that heavily leverage MRL. **Nomic Embed v1.5** (released Feb 2024) was a 768-dim English model finetuned with Matryoshka loss. It allowed output sizes down to 64 (even binary vectors via sign bit) and was shown to outperform OpenAI’s ada-002 at equivalent or smaller dimensionalities. Building on that, Nomic released **Embed Text V2** in 2025, a *multilingual* embedding model that uses a Mixture-of-Experts Transformer and Matryoshka training. Nomic v2 supports truncation from 768 → 256 dims with essentially no quality loss on BEIR/MIRACL benchmarks. In other words, at 256-dims it performs on par with its 768-dim version, which rivals much larger models. Nomic open-sourced the model weights and training code, making this one of the most accessible high-performance MRL models. These models are geared toward **semantic search and retrieval** (they use contrastive learning on web data and have “search\_document” style prompts), and they demonstrate that one can deploy a single model to different scenarios: e.g. use 256-dim embeddings for a memory-constrained application, or 768-dim for maximal accuracy – without retraining.

* **Voyage-3-large (Multilingual SOTA):** In 2025, startup Voyage AI announced `voyage-3-large`, a **general-purpose multilingual** embedding model that explicitly uses Matryoshka learning and quantization-aware training to push efficiency boundaries. Voyage-3-large produces 2048-dim embeddings, but is trained to also support 1024, 512, and 256 dimensions *from the same model*. They report that even a 256-dim embedding from this model outperforms OpenAI’s 3072-dim model on retrieval tasks by \~11% on average. Furthermore, by combining MRL with aggressive quantization (8-bit or binary), the model can reduce vector storage by up to 200× while *still* beating OpenAI’s accuracy. This is a cutting-edge example of the Matryoshka principle in action: **hierarchical embeddings** plus compression to optimize the accuracy-vs-cost tradeoff. (Voyage’s results show, for instance, that an int8 quantized 1024-dim vector is within 0.3% of the full 2048-dim float vector on retrieval quality.) While Voyage-3 is a commercial offering, it highlights how new entrants are leveraging MRL to challenge established players in the vector search space.

In summary, many **current semantic embedding models** follow the Matryoshka principle, even if they don’t always use that name. Open-source examples include sentence-transformer models (English and even Arabic) trained with `MatryoshkaLoss`, domain-specific embeddings like PubMedBERT and financial BGE with truncation support, and large-scale multilingual models like Alibaba’s **GTE** (dubbed “elastic embeddings”). On the commercial side, OpenAI’s latest text embeddings and Voyage’s model explicitly implement MRL for on-demand dimension scaling. The **core idea** is consistent across these: train the embedding space to be *coarse-to-fine*. This unlocks adaptive workflows in clustering, similarity search, and retrieval – where bulk operations can run on small, fast vectors, and only critical final steps use the full detail. As of mid-2025, Matryoshka-style resizable embeddings have moved from a research curiosity to a widely adopted technique in both open-source libraries and cutting-edge industry systems.

**Sources:**

* Kusupati et al., *“Matryoshka Representation Learning”*, NeurIPS 2022. (Introduces MRL; showed up to 14× smaller vectors with equal accuracy in vision and text models.)
* Hugging Face Blog – *“Introduction to Matryoshka Embedding Models”* by T. Aarsen et al., Feb 2024. (Concept overview and English MPNet Matryoshka example.)
* Sentence-Transformers Docs – *“Matryoshka Embeddings”*. (Training/inference details for MatryoshkaLoss and performance benchmarks.)
* Nomic AI Blog – *“Unboxing Nomic Embed v1.5: Resizable Embeddings”*, Feb 2024. (Open 768→64dim English model, vs. OpenAI and MiniLM benchmarks.)
* Nomic AI Blog – *“Embed Text V2 (Multilingual MoE with MRL)”*, 2025.
* OpenAI Product Update – *“New embedding models with lower pricing”*, Jan 2024. (Announcement of text-embedding-3 models with a `dimensions` parameter for truncation, powered by MRL.)
* Alibaba GTE on HuggingFace – *Model Card: gte-multilingual-base*, 2023. (305M encoder model, 70+ languages, elastic 128–768d output.)
* Milvus (Zilliz) Medium – *“Matryoshka Embeddings: Detail at Multiple Scales”*, 2024. (Discusses OpenAI, Nomic, Alibaba GTE as popular Matryoshka models.)
* Marqo Blog – *“MRL with CLIP for Multimodal Retrieval”*, 2025. (MRL applied to CLIP, showing minimal loss in truncated image/text embeddings.)
* Voyage AI Blog – *“voyage-3-large: SOTA general-purpose embedding model”*, Jan 2025. (Uses Matryoshka learning + quantization for efficient multilingual embeddings.)
